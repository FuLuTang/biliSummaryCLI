"""
è¯­éŸ³è½¬å†™æ¨¡å—
ä½¿ç”¨OpenAI Whisperè¿›è¡Œæœ¬åœ°è¯­éŸ³è½¬æ–‡å­—
"""
import os
import torch
import whisper
from typing import Callable, Optional
import concurrent.futures

class Transcriber:
    """è¯­éŸ³è½¬å†™å™¨"""
    
    # å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
    AVAILABLE_MODELS = [
        'tiny', 'base', 'small', 'medium', 'large', 'turbo',
        'gpt-4o-transcribe', 'gpt-4o-mini-transcribe', 'whisper-1'
    ]
    
    def __init__(self, model_name_or_path: str = 'base', api_key: Optional[str] = None, base_url: Optional[str] = None):
        """
        åˆå§‹åŒ–è½¬å†™å™¨
        
        Args:
            model_name_or_path: æ¨¡å‹åç§°æˆ–è·¯å¾„
            api_key: OpenAI API Key (å¦‚æœä½¿ç”¨APIè½¬å†™åˆ™å¿…é¡»)
            base_url: OpenAI Base URL
        """
        self.model = None
        self.model_name = model_name_or_path
        self.api_key = api_key
        self.base_url = base_url
        self.client = None
        
        # å…¼å®¹ turbo åç§°
        if self.model_name == 'turbo':
            self.model_name = 'large-v3-turbo'

    def _is_api_model(self):
        """æ£€æŸ¥æ˜¯å¦æ˜¯APIæ¨¡å‹"""
        return self.model_name in ['gpt-4o-transcribe', 'gpt-4o-mini-transcribe', 'whisper-1']

    def load_model(
        self,
        progress_callback: Optional[Callable[[float, str], None]] = None
    ):
        """åŠ è½½Whisperæ¨¡å‹"""
        if self._is_api_model():
            if not self.api_key:
                raise ValueError("ä½¿ç”¨åœ¨çº¿è½¬å†™æ¨¡å‹éœ€è¦æä¾› API Key")
            if self.client is None:
                from openai import OpenAI
                self.client = OpenAI(api_key=self.api_key, base_url=self.base_url)
            return

        if self.model is not None:
            return
        
        if progress_callback:
            progress_callback(75, f"åŠ è½½æ¨¡å‹: {self.model_name}...")
        
        try:
            # ç¡®å®šè®¾å¤‡
            # å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡å¼ºåˆ¶ä½¿ç”¨ CPU
            if os.environ.get('FORCE_CPU', '0') == '1':
                device = "cpu"
                print("âš ï¸ å·²å¼ºåˆ¶ä½¿ç”¨ CPU æ¨¡å¼")
            elif torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cuda" if torch.cuda.is_available() else "cpu"
            
            print(f"âœ“ Whisper è¿è¡Œè®¾å¤‡: {device.upper()}")
            
            # åŠ è½½æ¨¡å‹
            # in_memory=True å¯ä»¥ç¨å¾®åŠ é€ŸåŠ è½½ï¼Œä½†è´¹å†…å­˜
            self.model = whisper.load_model(self.model_name, device=device)
            
            if progress_callback:
                progress_callback(80, "æ¨¡å‹åŠ è½½å®Œæˆ")
                
        except Exception as e:
            raise Exception(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
    
    def transcribe(
        self,
        audio_path: str,
        language: Optional[str] = None,
        progress_callback: Optional[Callable[[float, str], None]] = None
    ) -> dict:
        """
        è½¬å†™éŸ³é¢‘æ–‡ä»¶
        """
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½ (æˆ–å®¢æˆ·ç«¯å·²åˆå§‹åŒ–)
        self.load_model(progress_callback)
        
        if progress_callback:
            progress_callback(82, "å¼€å§‹è¯­éŸ³è½¬å†™...")
        
        # --- API è½¬å†™è·¯å¾„ ---
        # --- API è½¬å†™è·¯å¾„ ---
        if self._is_api_model():
            # [Fix] æ£€æŸ¥éŸ³é¢‘æ—¶é•¿ï¼Œå¦‚æœè¶…è¿‡ 5 åˆ†é’Ÿå¼ºåˆ¶ä½¿ç”¨åˆ‡ç‰‡æ¨¡å¼
            # é˜²æ­¢ gpt-4o-mini ç­‰æ¨¡å‹åœ¨é•¿éŸ³é¢‘ä¸‹é™é»˜æˆªæ–­ (silent truncation)
            try:
                from core.audio_processor import AudioProcessor
                processor = AudioProcessor()
                duration = processor.get_audio_duration(audio_path)
                
                # [è°ƒæ•´] é˜ˆå€¼è®¾ä¸º 4åˆ†58ç§’ (298ç§’)
                # ç”¨æˆ·è¦æ±‚æ¯ 4åˆ†58ç§’ åˆ‡ç‰‡ï¼Œæ‰€ä»¥åªè¦è¶…è¿‡è¿™ä¸ªé•¿åº¦å°±è¿›å…¥åˆ‡ç‰‡æ¨¡å¼
                should_chunk = False
                if duration > 298: 
                    print(f"ğŸ“Š éŸ³é¢‘è¯¦æƒ…: æ—¶é•¿ {duration:.1f}s, å¤§å° {os.path.getsize(audio_path)/(1024*1024):.2f}MB")
                    print(f"âš ï¸ è§†é¢‘è¾ƒé•¿ï¼Œåˆ‡æ¢è‡³ã€å¹¶è¡Œåˆ†æ®µè½¬å†™ã€‘æ¨¡å¼ (æ¯ç‰‡ 4m58s)...")
                    should_chunk = True
                else:
                    print(f"ğŸ“Š éŸ³é¢‘è¯¦æƒ…: æ—¶é•¿ {duration:.1f}s, æ»¡è¶³å•æ¬¡è¯·æ±‚æ¡ä»¶")
            except Exception as e:
                print(f"âš ï¸ æ—¶é•¿æ£€æµ‹è®°å½•: {e}")
                should_chunk = False

            if should_chunk:
                if progress_callback:
                    progress_callback(83, f"æ­£åœ¨è¿›è¡Œåˆ†æ®µè½¬å†™ (æ€»é•¿ {duration:.1f}s)...")
                return self._transcribe_chunked(audio_path, self.model_name, language, progress_callback)

            try:
                if progress_callback:
                    progress_callback(85, f"æ­£åœ¨ä¸Šä¼ è‡³ OpenAI API è½¬å†™ (æ¨¡å‹: {self.model_name})...")
                
                print(f"ğŸ“¡ ä½¿ç”¨ OpenAI API è½¬å†™ (æ¨¡å‹: {self.model_name})")
                
                # gpt-4o-*-transcribe æ¨¡å‹ç›®å‰åªæ”¯æŒ json/text æ ¼å¼ï¼Œä¸æ”¯æŒ verbose_json
                response_fmt = "json" if self.model_name.startswith("gpt-4o") else "verbose_json"
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¦‚æœè¶…è¿‡ 24MBï¼Œè¿›è¡Œå‹ç¼©
                # import os  <-- remove this
                file_size_mb = os.path.getsize(audio_path) / (1024 * 1024)
                if file_size_mb > 24:
                    if progress_callback:
                        progress_callback(83, f"éŸ³é¢‘æ–‡ä»¶è¿‡å¤§ ({file_size_mb:.1f}MB)ï¼Œæ­£åœ¨å‹ç¼©...")
                    
                    from core.audio_processor import AudioProcessor
                    processor = AudioProcessor()
                    # å‹ç¼©ç”Ÿæˆæ–°æ–‡ä»¶
                    try:
                        audio_path = processor.compress_for_api(audio_path, progress_callback=progress_callback)
                    except Exception as e:
                        print(f"âš ï¸ å‹ç¼©å¤±è´¥: {e}ï¼Œå°†å°è¯•ç›´æ¥ä¸Šä¼ ...")

                with open(audio_path, "rb") as audio_file:
                    transcript = self.client.audio.transcriptions.create(
                        model=self.model_name, 
                        file=audio_file,
                        language=language,
                        response_format=response_fmt
                    )
                
                if progress_callback:
                    progress_callback(99, "API è½¬å†™å®Œæˆ")
                
                # å°è¯•æå–ä¿¡æ¯
                result = {
                    'text': transcript.text,
                    # JSON æ¨¡å¼ä¸‹æ²¡æœ‰è¿™äº›å…ƒæ•°æ®ï¼Œåªèƒ½ç»™é»˜è®¤å€¼
                    'language': getattr(transcript, 'language', 'auto'),
                    'duration': getattr(transcript, 'duration', 0.0)
                }
                
                # æ£€æŸ¥æ˜¯å¦æœ‰ usage ä¿¡æ¯
                if hasattr(transcript, 'usage') and transcript.usage:
                   result['usage'] = transcript.usage
                   print(f"ğŸ’° API æ¶ˆè€—ç»Ÿè®¡: {transcript.usage}")
                
                print(f"âœ… è½¬å†™æˆåŠŸ: æ”¶åˆ°æ–‡æœ¬ {len(transcript.text)} å­—ç¬¦")
                return result
                
            except Exception as e:
                error_str = str(e)
                
                # ç­–ç•¥ 1: å¦‚æœæ˜¯ Token è¶…é™æˆ–æ–‡ä»¶è¿‡å¤§ï¼Œè§¦å‘è‡ªåŠ¨åˆ‡ç‰‡
                if "input_too_large" in error_str or "maximum context" in error_str:
                    print(f"âš ï¸ API æŠ¥é”™: å†…å®¹è¿‡é•¿ ({error_str})")
                    print("ğŸ”„ è§¦å‘è‡ªåŠ¨åˆ‡ç‰‡è½¬å†™æ¨¡å¼ (Smart Chunking)...")
                    return self._transcribe_chunked(audio_path, self.model_name, language, progress_callback)

                # ç­–ç•¥ 2: å…¶ä»–é”™è¯¯ï¼Œå°è¯•å›é€€åˆ° whisper-1
                if self.model_name != "whisper-1":
                    print(f"âš ï¸ æ¨¡å‹ {self.model_name} è°ƒç”¨å¤±è´¥: {e}")
                    print("ğŸ”„ å°è¯•å›é€€åˆ°é€šç”¨æ¨¡å‹ whisper-1 ...")
                    try:
                        with open(audio_path, "rb") as audio_file:
                            transcript = self.client.audio.transcriptions.create(
                                model="whisper-1", 
                                file=audio_file,
                                language=language,
                                response_format="verbose_json"
                            )
                        return {
                            'text': transcript.text,
                            'language': getattr(transcript, 'language', 'auto'),
                            'duration': getattr(transcript, 'duration', 0.0)
                        }
                    except Exception as e2:
                        raise Exception(f"OpenAI API è½¬å†™å¤±è´¥ (å›é€€ä¹Ÿå¤±è´¥): {str(e2)}")
                
                raise Exception(f"OpenAI API è½¬å†™å¤±è´¥: {str(e)}")

        # --- æœ¬åœ° Whisper è½¬å†™è·¯å¾„ ---
        try:
            # å‡†å¤‡å‚æ•°
            # mps ä¸Š fp16 å¯èƒ½ä¼šå‡ºç° NaNï¼Œå¦‚æœé‡åˆ°é—®é¢˜å¯ä»¥è‡ªåŠ¨å›é€€
            # é»˜è®¤å°è¯•å¼€å¯ fp16 (åŒ…æ‹¬ MPS)
            fp16 = True
            
            # è½¬å†™
            result = self.model.transcribe(
                audio_path,
                language=language,
                verbose=False, # æˆ‘ä»¬è‡ªå·±æ‰“å°è¿›åº¦ï¼Œä¸ç”¨è‡ªå¸¦çš„
                fp16=fp16
            )
            
            if progress_callback:
                print(f"  - æ£€æµ‹åˆ°çš„è¯­è¨€: {result.get('language', 'unknown')}")
                print(f"  - æ–‡æœ¬é•¿åº¦: {len(result.get('text', ''))} å­—ç¬¦")
                progress_callback(99, "è½¬å†™å®Œæˆ")
            
            return result
            
        except RuntimeError as e:
            if "NaN" in str(e) and fp16:
                print("âš ï¸ æ£€æµ‹åˆ° NaN é”™è¯¯ï¼Œå°è¯•ç¦ç”¨ fp16 é‡è¯•...")
                return self.model.transcribe(
                    audio_path,
                    language=language,
                    fp16=False
                )
            raise e
        except Exception as e:
            raise Exception(f"è¯­éŸ³è½¬å†™å¤±è´¥: {str(e)}")
            
    def unload_model(self):
        """æ‰‹åŠ¨é‡Šæ”¾æ¨¡å‹å†…å­˜"""
        if self.model:
            del self.model
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect() 
            import torch
            if torch.backends.mps.is_available():
                torch.mps.empty_cache()
            elif torch.cuda.is_available():
                torch.cuda.empty_cache()
            self.model = None

    def format_segments(self, segments: list) -> str:
        """æ ¼å¼åŒ–åˆ†æ®µä¿¡æ¯ä¸ºå¸¦æ—¶é—´æˆ³çš„æ–‡æœ¬"""
        lines = []
        for seg in segments:
            start = self._format_time(seg.get('start', 0))
            end = self._format_time(seg.get('end', 0))
            text = seg.get('text', '').strip()
            lines.append(f"[{start} -> {end}] {text}")
        return '\n'.join(lines)
    
    @staticmethod
    def _format_time(seconds: float) -> str:
        """æ ¼å¼åŒ–æ—¶é—´"""
        minutes = int(seconds // 60)
        secs = int(seconds % 60)
        return f"{minutes:02d}:{secs:02d}"

    def _transcribe_part(self, i: int, total_chunks: int, chunk_path: str, model_name: str, language: str) -> dict:
        """è½¬å†™å•ä¸ªåˆ‡ç‰‡ (ç”¨äºå¹¶å‘)"""
        print(f"  -> [çº¿ç¨‹å¯åŠ¨] å¤„ç†ç‰‡æ®µ {i+1}/{total_chunks}...")
        try:
            with open(chunk_path, "rb") as audio_file:
                # å¹¶è¡Œæ¨¡å¼ä¸‹æ— æ³•ä½¿ç”¨ä¸Šæ–‡ context promptï¼Œå› ä¸ºä¸Šæ–‡è¿˜æ²¡å‡ºæ¥
                response = self.client.audio.transcriptions.create(
                    model=model_name,
                    file=audio_file,
                    language=language,
                    response_format="json"
                )
                print(f"  âˆš [ç‰‡æ®µå®Œæˆ] {i+1}/{total_chunks}")
                return {
                    "index": i,
                    "text": response.text,
                    "usage": getattr(response, 'usage', None)
                }
        except Exception as e:
            print(f"âš ï¸ [ç‰‡æ®µå¤±è´¥] {i+1}/{total_chunks}: {e}")
            raise e

    def _transcribe_chunked(self, audio_path: str, model_name: str, language: str, progress_callback: Optional[Callable] = None) -> dict:
        """
        åˆ†ç‰‡è½¬å†™é€»è¾‘ (å¹¶è¡ŒåŠ é€Ÿ)
        """
        from core.audio_processor import AudioProcessor
        processor = AudioProcessor()
        
        # 1. åˆ‡ç‰‡ (æ¯4åˆ†58ç§’ä¸€æ®µ = 298ç§’)
        chunk_duration = 298
        if progress_callback:
            progress_callback(84, f"æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ‡ç‰‡ (æ¯æ®µ {chunk_duration}s)...")
        
        chunks = processor.split_audio(audio_path, segment_seconds=chunk_duration)
        total_chunks = len(chunks)
        print(f"ğŸ”ª éŸ³é¢‘å·²åˆ‡åˆ†ä¸º {total_chunks} ä¸ªç‰‡æ®µ")
        
        results = [None] * total_chunks
        total_usage = {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0}
        
        # 2. å¹¶å‘è¯·æ±‚
        # é™åˆ¶å¹¶å‘æ•°ä¸º 4ï¼Œé¿å…è§¦å‘ API é€Ÿç‡é™åˆ¶ (429)
        max_workers = 4
        print(f"ğŸš€ å¯åŠ¨å¹¶å‘è½¬å†™ (å¹¶å‘æ•°: {max_workers})...")
        
        try:
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_index = {
                    executor.submit(
                        self._transcribe_part, i, total_chunks, chunk_path, model_name, language
                    ): i 
                    for i, chunk_path in enumerate(chunks)
                }
                
                completed_count = 0
                for future in concurrent.futures.as_completed(future_to_index):
                    index = future_to_index[future]
                    try:
                        # è·å–ç»“æœï¼Œå¦‚æœæœ‰å¼‚å¸¸ä¼šåœ¨è¿™é‡ŒæŠ›å‡º
                        data = future.result()
                        results[index] = data['text']
                        
                        # ç»Ÿè®¡ usage
                        if data['usage']:
                            u = data['usage']
                            # å…¼å®¹ä¸åŒå­—æ®µå (openai standard vs some compatible apis)
                            p_tokens = getattr(u, 'prompt_tokens', 0) or getattr(u, 'input_tokens', 0)
                            c_tokens = getattr(u, 'completion_tokens', 0) or getattr(u, 'output_tokens', 0)
                            
                            total_usage['prompt_tokens'] += p_tokens
                            total_usage['completion_tokens'] += c_tokens
                            total_usage['total_tokens'] += getattr(u, 'total_tokens', 0)
                            
                        completed_count += 1
                        if progress_callback:
                            progress = 85 + (completed_count / total_chunks) * 14
                            progress_callback(progress, f"å¹¶å‘è½¬å†™ä¸­... ({completed_count}/{total_chunks})")
                            
                    except Exception as e:
                        # ä»»ä½•ä¸€ä¸ªå¤±è´¥ï¼Œç›´æ¥ç»ˆæ­¢æ•´ä¸ªæµç¨‹
                        raise Exception(f"ç‰‡æ®µ {index+1} è½¬å†™å¤±è´¥ï¼Œæµç¨‹ç»ˆæ­¢ã€‚é”™è¯¯: {str(e)}")

        finally:
            # æ¸…ç†åˆ‡ç‰‡æ–‡ä»¶
            for chunk_path in chunks:
                if os.path.exists(chunk_path):
                    try:
                        os.remove(chunk_path)
                    except:
                        pass
        
        # 3. æ‹¼åˆç»“æœ
        # results åˆ—è¡¨å·²ç»æŒ‰ç…§ index ä½ç½®å¡«å……å¥½äº†
        # ç”¨æˆ·è¦æ±‚æ®µä¹‹é—´å¤šåŠ å›è½¦ï¼Œä½¿ç”¨ double newline åˆ†éš”
        combined_text = "\n\n".join(results)
        
        if progress_callback:
            progress_callback(99, "æ‰€æœ‰ç‰‡æ®µè½¬å†™æˆåŠŸï¼Œå·²åˆå¹¶")

        return {
            "text": combined_text,
            "language": language,
            "segments": [], 
            "usage": total_usage
        }
